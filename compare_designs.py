"""
Comparador de dise√±os usando Claude
Compara un dise√±o candidato contra un dise√±o de referencia
"""
import anthropic
import json
from config import Config

def compare_designs(reference_profile, candidate_profile):
    """
    Usa Claude para comparar dos perfiles de dise√±o
    
    Args:
        reference_profile: Perfil del dise√±o de referencia
        candidate_profile: Perfil del dise√±o a evaluar
    
    Returns:
        Dict con an√°lisis de desviaciones
    """
    client = anthropic.Anthropic(api_key=Config.ANTHROPIC_KEY)
    
    system_prompt = """Eres un asistente de comparaci√≥n de dise√±os UX/UI.

REGLAS IMPORTANTES:
1. NO juzgues dise√±os de forma aislada
2. SOLO compara el dise√±o candidato contra el dise√±o de referencia
3. Cita diferencias num√©ricas espec√≠ficas
4. Explica el impacto en la experiencia del usuario

Responde √öNICAMENTE con JSON v√°lido en este formato:
{
  "deviations": [
    {
      "area": "nombre espec√≠fico de la m√©trica",
      "reference_value": "valor exacto de la referencia",
      "candidate_value": "valor exacto del candidato",
      "impact": "explicaci√≥n breve del impacto en UX",
      "severity": "low|medium|high"
    }
  ],
  "overall_assessment": "evaluaci√≥n en una oraci√≥n",
  "comparison_confidence": "high|medium|low"
}

Si los perfiles son muy diferentes para comparar de forma significativa, usa "comparison_confidence": "low".
"""

    user_message = f"""Compara estos dos perfiles de dise√±o:

DISE√ëO DE REFERENCIA (comprobado):
{json.dumps(reference_profile, indent=2, ensure_ascii=False)}

DISE√ëO CANDIDATO (a evaluar):
{json.dumps(candidate_profile, indent=2, ensure_ascii=False)}

Identifica desviaciones significativas que podr√≠an impactar la experiencia del usuario."""

    try:
        message = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=2000,
            system=system_prompt,
            messages=[{"role": "user", "content": user_message}]
        )
        
        response_text = message.content[0].text
        
        # Limpiar respuesta (remover markdown si existe)
        if "```json" in response_text:
            response_text = response_text.split("```json")[1].split("```")[0]
        elif "```" in response_text:
            response_text = response_text.split("```")[1].split("```")[0]
        
        comparison = json.loads(response_text.strip())
        return comparison
        
    except Exception as e:
        print(f"‚ùå Error en comparaci√≥n con Claude: {e}")
        return {
            "deviations": [],
            "overall_assessment": "Error en el an√°lisis",
            "comparison_confidence": "low",
            "error": str(e)
        }

def calculate_score(reference_profile, candidate_profile, deviations):
    """
    Calcula un score determin√≠stico basado en desviaciones
    
    Args:
        reference_profile: Perfil de referencia
        candidate_profile: Perfil candidato
        deviations: Lista de desviaciones encontradas por Claude
    
    Returns:
        Float entre 0-10
    """
    score = 10.0
    
    # M√©tricas de texto
    ref_words = reference_profile['text_metrics']['avg_words_per_screen']
    cand_words = candidate_profile['text_metrics']['avg_words_per_screen']
    
    # Penalizar exceso de texto (carga cognitiva)
    if cand_words > ref_words * 2:
        score -= 2.0
        print(f"  ‚ö†Ô∏è  -2.0: Demasiado texto ({cand_words} vs {ref_words} palabras)")
    elif cand_words > ref_words * 1.5:
        score -= 1.0
        print(f"  ‚ö†Ô∏è  -1.0: Texto elevado ({cand_words} vs {ref_words} palabras)")
    elif cand_words < ref_words * 0.5:
        score -= 1.5
        print(f"  ‚ö†Ô∏è  -1.5: Muy poco texto ({cand_words} vs {ref_words} palabras)")
    
    # Penalizar falta de consistencia
    ref_consistent = reference_profile['interaction_metrics']['button_consistency']
    cand_consistent = candidate_profile['interaction_metrics']['button_consistency']
    
    if ref_consistent and not cand_consistent:
        score -= 1.5
        print(f"  ‚ö†Ô∏è  -1.5: Botones inconsistentes")
    
    # Penalizar falta de indicadores de progreso
    ref_progress = reference_profile['interaction_metrics']['progress_indicator_usage']
    cand_progress = candidate_profile['interaction_metrics']['progress_indicator_usage']
    
    if ref_progress > 0.5 and cand_progress == 0:
        score -= 1.0
        print(f"  ‚ö†Ô∏è  -1.0: Sin indicadores de progreso")
    
    # Penalizaciones basadas en severidad de Claude
    for dev in deviations:
        severity = dev.get('severity', 'low')
        if severity == 'high':
            score -= 1.0
            print(f"  ‚ö†Ô∏è  -1.0: {dev['area']} (severidad alta)")
        elif severity == 'medium':
            score -= 0.5
            print(f"  ‚ö†Ô∏è  -0.5: {dev['area']} (severidad media)")
    
    # Limitar entre 0 y 10
    final_score = max(0, min(10, score))
    
    return round(final_score, 1)

if __name__ == "__main__":
    # Test de comparaci√≥n
    print("\nüß™ Modo de prueba - Comparador de Dise√±os")
    print("="*60)
    
    # Cargar perfil de referencia
    try:
        with open(Config.REFERENCE_PROFILE_PATH, 'r') as f:
            ref_profile = json.load(f)
        print("‚úÖ Perfil de referencia cargado")
    except FileNotFoundError:
        print("‚ùå Perfil de referencia no encontrado")
        print("üí° Ejecuta: python generate_profile.py")
        exit(1)
    
    print("\nüìä Perfil de referencia:")
    print(f"  ‚Ä¢ Palabras promedio: {ref_profile['text_metrics']['avg_words_per_screen']}")
    print(f"  ‚Ä¢ Frames: {ref_profile['metadata']['total_frames']}")
    print(f"  ‚Ä¢ Consistencia: {ref_profile['interaction_metrics']['button_consistency']}")